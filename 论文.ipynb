{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280feead-60e9-4fde-b507-f9902f0709d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "将二进制代码文件转换为灰度图像数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5cd1b16-6007-4b54-a1cb-65b28be06bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def numerical_sort(value):\n",
    "    # 提取文件名中的数字\n",
    "    numbers = re.findall(r'\\d+', value)\n",
    "    return int(numbers[0]) if numbers else float('inf')  # 不符合的文件排序到最后\n",
    "\n",
    "def batch_convert_to_images(source_folder, target_folder):\n",
    "    # 确保目标文件夹存在\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "\n",
    "    # 获取源文件夹中所有文件并过滤\n",
    "    file_names = [f for f in os.listdir(source_folder) if re.match(r'二进制文件\\d+', f)]\n",
    "    # 使用自定义排序函数按数字顺序排序\n",
    "    file_names.sort(key=numerical_sort)\n",
    "\n",
    "    # 遍历所有文件并进行处理\n",
    "    for index, file_name in enumerate(file_names):\n",
    "        file_path = os.path.join(source_folder, file_name)\n",
    "        image_path = os.path.join(target_folder, f\"{index + 1}.bmp\")\n",
    "        \n",
    "        # 读取文件内容\n",
    "        with open(file_path, 'rb') as file:\n",
    "            file_bytes = file.read()\n",
    "\n",
    "        # 创建一个256x256的灰度图像的数组，初始化为0（全黑）\n",
    "        image_data = np.zeros((256, 256), dtype=np.uint8)\n",
    "\n",
    "        # 填充图像数组\n",
    "        for i in range(65536):  # 总是循环65536次，对应64KB\n",
    "            if i < len(file_bytes):\n",
    "                row = i // 256\n",
    "                col = i % 256\n",
    "                image_data[row, col] = file_bytes[i]\n",
    "            else:\n",
    "                break  # 如果file_bytes用完了，后面的image_data保持为0，无需显式填充\n",
    "\n",
    "        # 创建一个Pillow图像\n",
    "        img = Image.fromarray(image_data, 'L')\n",
    "        \n",
    "        # 保存图像\n",
    "        img.save(image_path)\n",
    "\n",
    "# 调用函数，处理文件\n",
    "batch_convert_to_images('VirusShare_x86-64_WinEXE_20130711', 'output_images_顺序正确_数量正确')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769514a4-1bdc-4111-a2d3-1a55997927ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "图像归一化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd8ad5b-b260-4e10-b9d1-3fe5f64ffacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预处理完成。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# 定义图像归一化函数\n",
    "def normalize_image(image_path, output_folder):\n",
    "    # 读取图像\n",
    "    img = Image.open(image_path).convert('L')  # 确保图像为灰度\n",
    "    # 归一化图像\n",
    "    normalized_img = np.array(img) / 255.0\n",
    "    # 从原始文件名中提取编号\n",
    "    file_name = os.path.basename(image_path)\n",
    "    base_name, _ = os.path.splitext(file_name)\n",
    "    # 保存图像到输出文件夹，使用.npy格式以保存浮点数像素值\n",
    "    save_path = os.path.join(output_folder, f\"{base_name}.npy\")\n",
    "    np.save(save_path, normalized_img)\n",
    "\n",
    "# 设置输入和输出文件夹路径\n",
    "input_folder = \"output_images_顺序正确_数量正确\"  # 替换为输入文件夹路径\n",
    "output_folder = \"归一化处理后_改变像素值特征\"  # 替换为输出文件夹路径\n",
    "\n",
    "# 创建输出文件夹\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 获取所有图像文件\n",
    "image_files = [f for f in os.listdir(input_folder) if f.endswith('.bmp')]\n",
    "\n",
    "# 对每个图像文件应用归一化预处理\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(input_folder, image_file)\n",
    "    normalize_image(image_path, output_folder)\n",
    "\n",
    "print(\"预处理完成。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7576ba0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有文件已处理并保存为Tensor格式。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "def numerical_sort(value):\n",
    "    # 使用正则表达式提取文件名中的数字，确保文件按数字顺序处理\n",
    "    numbers = re.findall(r'\\d+', value)\n",
    "    return int(numbers[0]) if numbers else float('inf')\n",
    "\n",
    "def convert_and_preprocess_images(source_folder, target_folder):\n",
    "    # 确保目标文件夹存在\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "\n",
    "    # 获取源文件夹中所有文件并过滤\n",
    "    file_names = [f for f in os.listdir(source_folder) if re.match(r'二进制文件\\d+', f)]\n",
    "    # 按数字顺序排序\n",
    "    file_names.sort(key=numerical_sort)\n",
    "\n",
    "    # 定义PyTorch图像预处理转换\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),  # 转为灰度图像\n",
    "        transforms.Resize((224, 224)),  # 调整图像大小到224x224\n",
    "        transforms.ToTensor(),  # 转为Tensor\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])  # 归一化\n",
    "    ])\n",
    "\n",
    "    # 遍历所有文件并进行处理\n",
    "    for index, file_name in enumerate(file_names):\n",
    "        file_path = os.path.join(source_folder, file_name)\n",
    "        with open(file_path, 'rb') as file:\n",
    "            file_bytes = file.read()\n",
    "\n",
    "        # 创建256x256的灰度图像数组，初始化为0（全黑）\n",
    "        image_data = np.zeros((256, 256), dtype=np.uint8)\n",
    "        for i in range(65536):  # 总是循环65536次，对应64KB\n",
    "            if i < len(file_bytes):\n",
    "                row = i // 256\n",
    "                col = i % 256\n",
    "                image_data[row, col] = file_bytes[i]\n",
    "\n",
    "        # 使用Pillow创建图像\n",
    "        img = Image.fromarray(image_data, 'L')\n",
    "\n",
    "        # 应用PyTorch预处理\n",
    "        img_tensor = transform(img)\n",
    "        \n",
    "        # 保存处理后的Tensor，此处为示例保存为.pt文件\n",
    "        tensor_path = os.path.join(target_folder, f\"{index + 1}.pt\")\n",
    "        torch.save(img_tensor, tensor_path)\n",
    "\n",
    "# 调用函数，处理文件\n",
    "source_folder = 'VirusShare_x86-64_WinEXE_20130711'\n",
    "target_folder = 'processed_images'\n",
    "convert_and_preprocess_images(source_folder, target_folder)\n",
    "print(\"所有文件已处理并保存为Tensor格式。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c31cbf",
   "metadata": {},
   "source": [
    "接下来的目标是提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d718f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 PyTorch 库,这是一个用于机器学习和深度学习的开源框架。\n",
    "import torch\n",
    "\n",
    "# 导入 PyTorch 的神经网络模块 nn\n",
    "import torch.nn as nn\n",
    "\n",
    "# 导入 PyTorch 的优化器模块 optim\n",
    "import torch.optim as optim\n",
    "\n",
    "# 导入 torchvision 库，它提供了常用的数据集和图像转换工具\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 从 torchvision.models 导入 vgg16 模型，vgg16 是一种常用的图像识别模型\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "# 导入 DataLoader，它提供了对 Dataset 的迭代访问\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# 设定基本参数\n",
    "batch_size = 32  # 批处理大小\n",
    "learning_rate = 0.001  # 学习率\n",
    "num_epochs = 20  # 训练周期\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 设定使用GPU或CPU\n",
    "\n",
    "# 数据预处理，这里只使用灰度图像，所以只有一个通道\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # 转为灰度图像\n",
    "    transforms.Resize((224, 224)),  # 调整图像大小到224x224\n",
    "    transforms.ToTensor(),  # 转为Tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # 归一化\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "train_dataset = datasets.ImageFolder(root='path_to_train_dataset', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='path_to_test_dataset', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 修改VGG16模型以接受1通道输入并进行一些调整\n",
    "class VGG16Gray(nn.Module):\n",
    "    def __init__(self, num_classes=2):  # 默认为二分类\n",
    "        super(VGG16Gray, self).__init__()\n",
    "        original_vgg = vgg16(pretrained=True)\n",
    "        # 替换原始VGG的第一层，从3通道改为1通道\n",
    "        self.features = original_vgg.features\n",
    "        self.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.classifier = original_vgg.classifier\n",
    "        self.classifier[6] = nn.Linear(4096, num_classes)  # 替换分类器的最后一层为目标类别数\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# 实例化模型\n",
    "model = VGG16Gray().to(device)\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "def train_model():\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 测试模型\n",
    "def test_model():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')\n",
    "\n",
    "# 执行训练和测试\n",
    "train_model()\n",
    "test_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c99bb2",
   "metadata": {},
   "source": [
    "整合\n",
    "\n",
    "修改了保存为npy格式，改为保存为张量格式，不保存为npy格式，能够更好的适应pytorch的需求\n",
    "\n",
    "具体如下\n",
    "\n",
    "上述整合代码与PyTorch中的数据预处理代码实现的功能基本相似，但存在几个关键的区别和特点：\n",
    "\n",
    "1. **技术栈和应用场景**：\n",
    "   - **PyTorch 数据预处理**：适用于在PyTorch框架中处理图像数据，主要用于训练机器学习模型。它利用PyTorch的库和函数来转换图像，包括调整大小、转换为张量并归一化。\n",
    "   - **整合代码**：处理的是二进制文件，将其直接转换为灰度图像的numpy数组，并进行归一化处理。这种处理通常是为了准备从非标准图像源（如二进制文件）导入的数据，适用于特定应用，比如处理二进制恶意软件样本。\n",
    "\n",
    "2. **输出格式**：\n",
    "   - **PyTorch 数据预处理**：输出是归一化后的Tensor，直接适用于PyTorch模型。\n",
    "   - **整合代码**：输出是.npy格式的文件，这是一个包含归一化后的图像数据的numpy数组，可用于多种不同的数据科学和机器学习应用，不限于PyTorch。\n",
    "\n",
    "3. **处理流程**：\n",
    "   - **PyTorch 数据预处理**：不涉及从原始二进制文件生成图像的过程，假设输入已经是图像格式。\n",
    "   - **整合代码**：包含从原始二进制文件读取数据并转换为图像的步骤，这是一个更底层的数据处理，特别是在处理非图像数据文件时非常有用。\n",
    "\n",
    "4. **实用性和灵活性**：\n",
    "   - **PyTorch 数据预处理**：是标准化流程，用于图像数据，并且严重依赖PyTorch环境。\n",
    "   - **整合代码**：更通用，可以在没有PyTorch环境的情况下运行，因为它使用Python标准库和Pillow进行图像处理，适合需要从基础数据格式开始的预处理任务。\n",
    "\n",
    "总的来说，两者虽然在最终目标（即为机器学习模型准备图像数据）上相似，但处理的起点、依赖的库、输出格式以及在实际应用中的灵活性上有所不同。如果你的工作流程需要从非标准图像源（例如二进制文件）处理数据，整合代码提供了一个很好的起点。如果是标准图像文件，并且使用PyTorch进行深度学习，那么PyTorch的预处理流程更直接有效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a13cf4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "所有文件已处理并保存为Tensor格式。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /Users/guo/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100.0%\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'processed_images/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 130\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# 加载数据\u001b[39;00m\n\u001b[1;32m    124\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m    125\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[1;32m    126\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m    127\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m])\n\u001b[1;32m    128\u001b[0m ])\n\u001b[0;32m--> 130\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessed_images/train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_images/test\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m    133\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/datasets/folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    321\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    327\u001b[0m ):\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/datasets/folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[1;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/datasets/folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/datasets/folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'processed_images/train'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ssl\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "import urllib.request\n",
    "\n",
    "# 创建一个不验证SSL证书的上下文\n",
    "context = ssl._create_unverified_context()\n",
    "\n",
    "# 使用这个上下文发起请求\n",
    "response = urllib.request.urlopen('https://example.com', context=context)\n",
    "print(response.read().decode())\n",
    "\n",
    "\n",
    "def numerical_sort(value):\n",
    "    # 使用正则表达式提取文件名中的数字，确保文件按数字顺序处理\n",
    "    numbers = re.findall(r'\\d+', value)\n",
    "    return int(numbers[0]) if numbers else float('inf')\n",
    "\n",
    "def convert_and_preprocess_images(source_folder, target_folder):\n",
    "    # 确保目标文件夹存在\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "\n",
    "    # 获取源文件夹中所有文件并过滤\n",
    "    file_names = [f for f in os.listdir(source_folder) if re.match(r'二进制文件\\d+', f)]\n",
    "    # 按数字顺序排序\n",
    "    file_names.sort(key=numerical_sort)\n",
    "\n",
    "    # 定义图像预处理转换\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),  # 转为灰度图像\n",
    "        transforms.Resize((224, 224)),  # 调整图像大小到224x224\n",
    "        transforms.ToTensor(),  # 转为Tensor\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])  # 归一化\n",
    "    ])\n",
    "\n",
    "    # 遍历所有文件并进行处理\n",
    "    for index, file_name in enumerate(file_names):\n",
    "        file_path = os.path.join(source_folder, file_name)\n",
    "        with open(file_path, 'rb') as file:\n",
    "            file_bytes = file.read()\n",
    "\n",
    "        # 创建256x256的灰度图像数组，初始化为0（全黑）\n",
    "        image_data = np.zeros((256, 256), dtype=np.uint8)\n",
    "        for i in range(65536):\n",
    "            if i < len(file_bytes):\n",
    "                row = i // 256\n",
    "                col = i % 256\n",
    "                image_data[row, col] = file_bytes[i]\n",
    "\n",
    "        # 使用Pillow创建图像\n",
    "        img = Image.fromarray(image_data, 'L')\n",
    "        img_tensor = transform(img)\n",
    "        tensor_path = os.path.join(target_folder, f\"{index + 1}.pt\")\n",
    "        torch.save(img_tensor, tensor_path)\n",
    "\n",
    "# 调用函数，处理文件\n",
    "source_folder = 'VirusShare_x86-64_WinEXE_20130711'\n",
    "target_folder = 'processed_images'\n",
    "convert_and_preprocess_images(source_folder, target_folder)\n",
    "print(\"所有文件已处理并保存为Tensor格式。\")\n",
    "\n",
    "# 模型训练与测试\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 修改VGG16模型以接受1通道输入\n",
    "class VGG16Gray(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(VGG16Gray, self).__init__()\n",
    "        original_vgg = models.vgg16(pretrained=True)\n",
    "        self.features = original_vgg.features\n",
    "        self.features[0] = nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.classifier = original_vgg.classifier\n",
    "        self.classifier[6] = nn.Linear(4096, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = VGG16Gray().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练模型\n",
    "def train_model():\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 测试模型\n",
    "def test_model():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy on test set: {(correct / total) * 100:.2f}%')\n",
    "\n",
    "# 加载数据\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='processed_images/train', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root='processed_images/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 设置超参数\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# 训练并测试模型\n",
    "train_model()\n",
    "test_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4093a0b2",
   "metadata": {},
   "source": [
    "#收集所有的良性文件\n",
    "从本地电脑爬取，然后保存下来，下面是代码\n",
    "目录是c盘\n",
    "限定大小为大于2kb小于100mb\n",
    "处理了错误，跳过访问权限错误和找不到的错误\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0aa2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_exe_files(src_folder, dst_folder):\n",
    "    # 确保目标目录存在\n",
    "    if not os.path.exists(dst_folder):\n",
    "        os.makedirs(dst_folder)\n",
    "\n",
    "    file_count = 0  # 初始化文件计数器\n",
    "    for root, dirs, files in os.walk(src_folder):\n",
    "        dirs[:] = [d for d in dirs if os.access(os.path.join(root, d), os.R_OK)]  # 检查目录读取权限\n",
    "        for file in files:\n",
    "            if file.endswith('.exe'):\n",
    "                src_file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    file_size = os.path.getsize(src_file_path)\n",
    "                    if file_size < 2048 or file_size > 104857600:  # 跳过小于2KB或大于100MB的文件\n",
    "                        continue\n",
    "                except OSError:\n",
    "                    continue  # 如果无法获取文件大小，则跳过此文件\n",
    "\n",
    "                dst_file_path = os.path.join(dst_folder, file)  # 直接在目标文件夹下保存文件\n",
    "                if not os.path.exists(dst_file_path):  # 检查是否已存在同名文件\n",
    "                    try:\n",
    "                        shutil.copy(src_file_path, dst_file_path)\n",
    "                        file_count += 1  # 文件计数增加\n",
    "                        print(f\"Copied: {src_file_path} to {dst_file_path}\")\n",
    "                    except (PermissionError, FileNotFoundError):\n",
    "                        continue  # 如果没有权限复制文件或文件不存在，则跳过\n",
    "\n",
    "    print(f\"Total .exe files copied: {file_count}\")\n",
    "\n",
    "dir_src = \"C:\\\\\"  # C盘根目录\n",
    "dir_dst = \"C:\\\\BenignFiles\"  # 目标目录\n",
    "copy_exe_files(dir_src, dir_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8472ba30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DefaultVerifyPaths(cafile='/Library/Frameworks/Python.framework/Versions/3.12/etc/openssl/cert.pem', capath=None, openssl_cafile_env='SSL_CERT_FILE', openssl_cafile='/Library/Frameworks/Python.framework/Versions/3.12/etc/openssl/cert.pem', openssl_capath_env='SSL_CERT_DIR', openssl_capath='/Library/Frameworks/Python.framework/Versions/3.12/etc/openssl/certs')\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "print(ssl.get_default_verify_paths())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9164e1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests.exceptions import SSLError\n",
    "\n",
    "try:\n",
    "    response = requests.get('https://example.com', verify='/Library/Frameworks/Python.framework/Versions/3.12/etc/openssl/cert.pem')\n",
    "    print(response.text)\n",
    "except SSLError as e:\n",
    "    print(\"SSL Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd51ad",
   "metadata": {},
   "source": [
    "仅仅作为特征提取器\n",
    "修改\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be33c15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "所有文件已处理并保存为Tensor格式。\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any class folder in processed_images.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 73\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\n\u001b[1;32m     67\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     68\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[1;32m     69\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     70\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m])\n\u001b[1;32m     71\u001b[0m ])\n\u001b[0;32m---> 73\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessed_images\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# 提取特征\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/datasets/folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    321\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    327\u001b[0m ):\n\u001b[0;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/datasets/folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[1;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/datasets/folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/datasets/folder.py:43\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     41\u001b[0m classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mscandir(directory) \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m class_to_idx \u001b[38;5;241m=\u001b[39m {cls_name: i \u001b[38;5;28;01mfor\u001b[39;00m i, cls_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classes)}\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m classes, class_to_idx\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find any class folder in processed_images."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ssl\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "import urllib.request\n",
    "\n",
    "# 创建一个验证SSL证书的上下文\n",
    "context = ssl.create_default_context()\n",
    "\n",
    "# 使用这个上下文发起请求\n",
    "response = urllib.request.urlopen('https://example.com', context=context)\n",
    "print(response.read().decode())\n",
    "\n",
    "def numerical_sort(value):\n",
    "    numbers = re.findall(r'\\d+', value)\n",
    "    return int(numbers[0]) if numbers else float('inf')\n",
    "\n",
    "def convert_and_preprocess_images(source_folder, target_folder):\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "    file_names = [f for f in os.listdir(source_folder) if re.match(r'文件\\d+', f)]\n",
    "    file_names.sort(key=numerical_sort)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    for index, file_name in enumerate(file_names):\n",
    "        file_path = os.path.join(source_folder, file_name)\n",
    "        with open(file_path, 'rb') as file:\n",
    "            file_bytes = file.read()\n",
    "        image_data = np.zeros((256, 256), dtype=np.uint8)\n",
    "        for i in range(min(65536, len(file_bytes))):\n",
    "            row = i // 256\n",
    "            col = i % 256\n",
    "            image_data[row, col] = file_bytes[i]\n",
    "        img = Image.fromarray(image_data, 'L')\n",
    "        img_tensor = transform(img)\n",
    "        tensor_path = os.path.join(target_folder, f\"{index + 1}.pt\")\n",
    "        torch.save(img_tensor, tensor_path)\n",
    "    print(\"所有文件已处理并保存为Tensor格式。\")\n",
    "\n",
    "source_folder = 'VirusShare_x86-64_WinEXE_20130711'\n",
    "target_folder = 'processed_images'\n",
    "convert_and_preprocess_images(source_folder, target_folder)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载预训练的 VGG16 模型，但只使用卷积层作为特征提取器\n",
    "model = models.vgg16(pretrained=True).features.to(device)\n",
    "model.eval()\n",
    "\n",
    "def extract_features(data_loader):\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            features.extend(outputs.cpu().numpy())\n",
    "    return features\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root='processed_images', transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 提取特征\n",
    "features = extract_features(loader)\n",
    "print(\"特征提取完成，特征维度：\", features[0].shape)\n",
    "\n",
    "# features 现在包含了每个图像通过 VGG 最后一个卷积层的输出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b467e449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征提取完成，特征维度： (1000,)\n",
      "总共提取了 997 组特征。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载预训练的 VGG16 模型\n",
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "# 修改第一个卷积层以接受1通道输入\n",
    "model.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1).to(device)\n",
    "\n",
    "# 使用卷积层部分作为特征提取器\n",
    "model.features = model.features.to(device)\n",
    "model.eval()\n",
    "\n",
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        super(TensorDataset, self).__init__()\n",
    "        self.file_paths = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.pt')]\n",
    "        self.file_paths.sort(key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor = torch.load(self.file_paths[idx])\n",
    "        return tensor, 0  # 返回0作为伪标签，因为不进行分类\n",
    "\n",
    "def extract_features(data_loader):\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for tensors, _ in data_loader:\n",
    "            tensors = tensors.to(device)\n",
    "            outputs = model(tensors)\n",
    "            features.extend(outputs.cpu().numpy())\n",
    "    return features\n",
    "\n",
    "# 初始化数据集和数据加载器\n",
    "dataset = TensorDataset('processed_images')\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 提取特征\n",
    "features = extract_features(loader)\n",
    "print(\"特征提取完成，特征维度：\", features[0].shape if features else 'No features extracted')\n",
    "\n",
    "# 输出信息，确保特征已被提取\n",
    "if features:\n",
    "    print(f\"总共提取了 {len(features)} 组特征。\")\n",
    "else:\n",
    "    print(\"未提取任何特征，请检查数据文件。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eaf63aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征已成功保存至 'extracted_features.npy'\n",
      "特征提取完成，特征维度： (1000,)\n",
      "总共提取了 997 组特征。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 加载预训练的 VGG16 模型\n",
    "model = models.vgg16(pretrained=True)\n",
    "# 修改第一个卷积层以接受1通道输入\n",
    "model.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1).to(device)\n",
    "# 使用卷积层部分作为特征提取器\n",
    "model.features = model.features.to(device)\n",
    "model.eval()\n",
    "\n",
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        super(TensorDataset, self).__init__()\n",
    "        self.file_paths = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.pt')]\n",
    "        self.file_paths.sort(key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor = torch.load(self.file_paths[idx])\n",
    "        return tensor, 0  # 返回0作为伪标签，因为不进行分类\n",
    "\n",
    "def extract_features(data_loader):\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for tensors, _ in data_loader:\n",
    "            tensors = tensors.to(device)\n",
    "            outputs = model(tensors)\n",
    "            features.extend(outputs.cpu().numpy())\n",
    "    return features\n",
    "\n",
    "# 初始化数据集和数据加载器\n",
    "dataset = TensorDataset('processed_images')\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 提取特征\n",
    "features = extract_features(loader)\n",
    "\n",
    "# 添加的部分：保存提取的特征\n",
    "if features:\n",
    "    np.save('extracted_features.npy', np.array(features))\n",
    "    print(\"特征已成功保存至 'extracted_features.npy'\")\n",
    "    print(\"特征提取完成，特征维度：\", features[0].shape)\n",
    "    print(f\"总共提取了 {len(features)} 组特征。\")\n",
    "else:\n",
    "    print(\"未提取任何特征，请检查数据文件。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
